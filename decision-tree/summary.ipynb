{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树概念与基本流程\n",
    "\n",
    "决策树（decision tree）是一种基本的分类与回归算法，这里主要讨论用于分类的决策树。\n",
    "\n",
    "优点：\n",
    "\n",
    "- 模型具有可读性\n",
    "- 分类速度快\n",
    "\n",
    "决策树学习通常包括3个步骤：\n",
    "\n",
    "1. 特征选择\n",
    "2. 决策树的生成\n",
    "3. 决策树的修剪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概念\n",
    "\n",
    "一般的，一棵决策树包含一个**根结点**、若干个**内部结点**和若干个**叶结点**；**叶结点**对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的**路径**对应了一个判定测试序列。\n",
    "\n",
    "例如，周志华老师的《机器学习》中的西瓜例子，叶子结点对应了决策结果（好瓜、坏瓜）；根结点（纹理）和内部结点（根蒂、触感、色泽等）对应了属性测试。\n",
    "\n",
    "<img src=\"./decision-tree.jpg\" width=\"400\" />\n",
    "\n",
    "决策树学习的**目的**是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习过程\n",
    "\n",
    "\n",
    "决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一颗决策树。\n",
    "\n",
    "周志华老师《机器学习》中决策树学习基本算法的伪代码挺好的，摘录如下（上面描述中的“特征”等价于下图中的“属性”）：\n",
    "\n",
    "<img src=\"./decision-tree-algrithom.jpg\" width=\"600\">\n",
    "\n",
    "可以看出，学习过程中有三种返回情形：\n",
    "\n",
    "1. D中的样本全部属于同一类别\n",
    "2. 属性集A为空集或者D中样本在A上取值相同\n",
    "3. $D_v$ 为空\n",
    "\n",
    "当然，这样得到的决策树很有可能过拟合，还需要进行“剪枝”。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
