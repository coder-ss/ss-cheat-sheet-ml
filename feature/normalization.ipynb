{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征归一化\n",
    "\n",
    "归一化（Normalization）是特征处理过程中经常使用的关键一步。经常也叫Scaling。\n",
    "\n",
    "同一个样本，可能具备不同类型的特征，各特征的数值大小范围不一致。所谓特征归一化，就是将不同类型的特征数值大小变为一致的过程。\n",
    "\n",
    "## 为什么需要\n",
    "\n",
    "特征归一化的好处有：\n",
    "\n",
    "1. 归一化后加快了梯度下降求最优解的速度；\n",
    "2. 归一化有可能提高精度。\n",
    "\n",
    "针对上面两个好处，下面给出具体的解释。\n",
    "\n",
    "**1）归一化为什么能提高梯度下降法求解最优解的速度？**\n",
    "\n",
    "如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；\n",
    "\n",
    "而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。\n",
    "\n",
    "因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。\n",
    "\n",
    "<img src=\"./normalization.jpg\" width=\"480\">\n",
    "\n",
    "**2）归一化为什么有可能提高精度？**\n",
    "\n",
    "一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化方法\n",
    "\n",
    "### 线性归一化（Min-Max Scaling）\n",
    "\n",
    "线性函数将原始数据线性化的方法转换到[0, 1]的范围，归一化公式如下：\n",
    "\n",
    "$$ \\boldsymbol{x}^{\\prime} = \\frac{\\boldsymbol{x} - \\min(\\boldsymbol{x})}{\\max(\\boldsymbol{x}) - \\min(\\boldsymbol{x})} $$\n",
    "\n",
    "该方法实现对原始数据的等比例缩放。\n",
    "\n",
    "这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。\n",
    "\n",
    "sklearn中有相应的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ],\n",
       "       [ 1.        ,  0.33333333],\n",
       "       [ 0.33344409,  0.66666667],\n",
       "       [ 0.52001994,  1.        ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x=[[10001,2],[16020,4],[12008,6],[13131,8]]\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准差标准化（Standardization）\n",
    "\n",
    "又称为Z-score normalization。\n",
    "\n",
    "经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：\n",
    "\n",
    "$$ \\boldsymbol{x}^{*} = \\frac{\\boldsymbol{x} - \\mu}{\\sigma} $$\n",
    "\n",
    "其中μ为所有样本数据的均值，σ为所有样本数据的标准差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2817325 , -1.34164079],\n",
       "       [ 1.48440157, -0.4472136 ],\n",
       "       [-0.35938143,  0.4472136 ],\n",
       "       [ 0.15671236,  1.34164079]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "x=[[10001,2],[16020,4],[12008,6],[13131,8]]\n",
    "X_scaler = StandardScaler()\n",
    "X_scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两种方法的选择\n",
    "\n",
    "1. 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，第二种方法(Z-score standardization)表现更好。\n",
    "2. 在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
