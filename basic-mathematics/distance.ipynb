{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 距离和相似度\n",
    "\n",
    "很多机器学习算法都会要计算样本之间或特征之间的相似度、距离，因此，对相似度、距离有一个全面的认识是非常有好处的。\n",
    "\n",
    "\n",
    "## 闵可夫斯基距离\n",
    "\n",
    "可能没怎么听过闵可夫斯基距离（Minkowski distance），但只要知道欧式距离、曼哈顿距离都是它的特例，你就知道为什么我要首先介绍闵可夫斯基距离了。\n",
    "\n",
    "一般我们描述一个向量时，表示成列向量的形式，即 $\\boldsymbol{a} = (a_1, a_2, \\ldots, a_n)^\\text{T}$。两个n维向量 $\\boldsymbol{a}$、$\\boldsymbol{b}$ 之间的闵可夫斯基距离为：\n",
    "\n",
    "$$ d = \\left( \\sum_{i=1}^{n} \\mid a_i - b_i \\mid^{p} \\right)^{\\frac{1}{p}} $$\n",
    "\n",
    "即为 $\\boldsymbol{a} - \\boldsymbol{b}$ 的 $L_p$ 范数 $\\left\\| \\boldsymbol{a} - \\boldsymbol{b} \\right\\|_p$\n",
    "。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欧氏距离\n",
    "\n",
    "欧氏距离（Euclidean Distance）源自于欧氏空间中两点的距离。更一般的情况是将点换成更高维的向量。\n",
    "\n",
    "平面内两点之间 $a = (a_x, a_y)$、$b = (b_x, b_y)$ 的欧氏距离：\n",
    "\n",
    "$$ d = \\sqrt{\\left( a_x - b_x \\right)^2 + \\left( a_y - b_y \\right)^2 } $$\n",
    "\n",
    "两个n维向量 $\\boldsymbol{a}$、$\\boldsymbol{b}$ 之间的欧氏距离为：\n",
    "\n",
    "$$ d = \\sqrt{\\sum_{i=1}^{n} \\left( a_i - b_i\\right)^2} = \\sqrt{\\left( \\boldsymbol{a} - \\boldsymbol{b} \\right)^{\\text{T}} \\left( \\boldsymbol{a} - \\boldsymbol{b} \\right)} $$\n",
    "\n",
    "可以看出，欧式距离就是闵可夫斯基距离中p=2的特例。\n",
    "\n",
    "根据定义，我们可以用numpy.linalg.norm来计算欧式距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8284271247461903"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linalg.norm(np.array([1, 2]) - np.array([3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当要计算多个向量两两之间的欧氏距离时，可以用sklearn提供的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  2.82842712,  5.65685425],\n",
       "       [ 2.82842712,  0.        ,  2.82842712],\n",
       "       [ 5.65685425,  2.82842712,  0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "euclidean_distances(X, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面结果的第一行分别是向量 [1, 2] 与 [1, 2]、[3, 4]、[5, 6] 的欧氏距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 曼哈顿距离\n",
    "\n",
    "曼哈顿距离（Manhattan distance）又叫街区距离（city block distance），是闵可夫斯基距离中p=1的特例：\n",
    "\n",
    "$$ d = \\sum_{i=1}^{n} \\mid a_i - b_i \\mid $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard相似度\n",
    "\n",
    "杰卡德相似度（Jaccard similarity coefficient），也叫Jaccard index或Intersection over Union。它是衡量两个集合相似度的指标，计算方式如下：\n",
    "\n",
    "$$ J(A, B) = \\frac{\\mid A \\cap B \\mid}{\\mid A \\cup B \\mid} = \\frac{\\mid A \\cap B \\mid}{\\mid A \\mid + \\mid B \\mid - \\mid A \\cap B \\mid} $$\n",
    "\n",
    "当A、B都为空集时，定义Jaccard相似度等于1。Jaccard相似度的取值始终在闭区间[0, 1]上。\n",
    "\n",
    "杰卡德距离（Jaccard distance）是与Jaccard相似度相反的概念：\n",
    "\n",
    "$$ d_{J}(A, B) = 1 - J(A, B) = \\frac{\\mid A \\cup B \\mid - \\mid A \\cap B \\mid}{\\mid A \\cup B \\mid} $$\n",
    "\n",
    "贴个图，直观感受下：\n",
    "\n",
    "<img src=\"./jaccard.png\" />\n",
    "\n",
    "sklearn计算Jaccard相似度的函数如下，下面的例子有5个样本，有3个样本预测正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59999999999999998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "y_pred = [0, 2, 1, 3, 1]\n",
    "y_true = [0, 1, 2, 3, 1]\n",
    "jaccard_similarity_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 余弦相似度\n",
    "\n",
    "几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。它的计算方式如下：\n",
    "\n",
    "$$ \\cos(\\theta) = \\frac{\\boldsymbol{a}^\\text{T} \\boldsymbol{b}}{\\mid \\boldsymbol{a} \\mid \\cdot \\mid \\boldsymbol{b} \\mid} = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum_{i=1}^{n} a_i^2} \\sqrt{\\sum_{i=1}^{n} b_i^2}} $$\n",
    "\n",
    "夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。\n",
    "\n",
    "sklearn中用cosine_similarity函数计算多个向量两两之间的余弦相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.70710678,  0.        , -0.70710678, -1.        ],\n",
       "       [ 0.70710678,  1.        ,  0.70710678,  0.        , -0.70710678],\n",
       "       [ 0.        ,  0.70710678,  1.        ,  0.70710678,  0.        ],\n",
       "       [-0.70710678,  0.        ,  0.70710678,  1.        ,  0.70710678],\n",
       "       [-1.        , -0.70710678,  0.        ,  0.70710678,  1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X = [[1, 0], [1, 1], [0, 1], [-1, 1], [-1, 0]]\n",
    "cosine_similarity(X, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pearson相似度\n",
    "\n",
    "$$ r = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\overline{x}) (y_i - \\overline{y}) }{\\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 } \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\overline{y})^2 } } = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x}) (y_i - \\overline{y}) }{\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2 } \\sqrt{\\sum_{i=1}^{n} (y_i - \\overline{y})^2 } }  $$\n",
    "\n",
    "其中 $\\overline{x}$ 是 $\\boldsymbol{x}$ 的均值。\n",
    "\n",
    "可以看出Pearson相似度与余弦相似度的区别就在于计算过程中是否减去均值（叫去中心化）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.14499815458068518, 0.68940144811669546)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinregressResult(slope=0.20833333333333329, intercept=13.375, rvalue=0.14499815458068518, pvalue=0.68940144811669501, stderr=0.50261704627083648)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "a = [15, 12, 8, 8, 7, 7, 7, 6, 5, 3]\n",
    "b = [10, 25, 17, 11, 13, 17, 20, 13, 9, 15]\n",
    "print(pearsonr(a, b))\n",
    "\n",
    "# slope : slope of the regression line\n",
    "# intercept : intercept of the regression line\n",
    "# r-value : correlation coefficient\n",
    "# p-value : two-sided p-value for a hypothesis test whose null hypothesis is that the slope is zero\n",
    "# stderr : Standard error of the estimate\n",
    "linregress(a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
