{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 误差逆传播算法\n",
    "\n",
    "误差逆传播算法（error BackPropagation，简称BP）算法也叫“反向传播算法”。\n",
    "\n",
    "给定训练集 $D = \\{ (\\boldsymbol{x}_1, \\boldsymbol{y}_1), (\\boldsymbol{x}_2, \\boldsymbol{y}_2), \\ldots, (\\boldsymbol{x}_m, \\boldsymbol{y}_m) \\},\\ \\boldsymbol{x}_i \\in \\mathbb{R}^d, \\ \\boldsymbol{y}_i \\in \\mathbb{R}^l$，即输入示例由d个属性描述，输出l维实值向量。下图给出了一个拥有d个输入神经元、l个输出神经元、q个隐层神经元的多层前馈网络结构，其中\n",
    "\n",
    "- $\\theta_j$ 表示输出层第j个神经元的阈值\n",
    "- $\\gamma_h$ 表示隐层第h个神经元的阈值\n",
    "- $v_{ih}$ 表示输入层第i个神经元与隐层第h个神经元之间的连接权\n",
    "- $w_{hj}$ 表示隐层第h个神经元与输出层第j个神经元之间的连接权\n",
    "\n",
    "记\n",
    "\n",
    "- $\\alpha_h = \\sum_{i=1}^{d} v_{ih}x_i$ 表示隐层第h个神经元接收到的输入\n",
    "- $\\beta_j = \\sum{h=1}^{q} w_{hj}b_h$ 表示输出层第j个神经元接收到的输入，其中 $b_h$ 为隐层第h个神经元的输出。\n",
    "\n",
    "假设隐层和输出层的激活函数都使用Sigmoid函数 $f = \\frac{1}{1 + e^{-x}}$ 。\n",
    "\n",
    "<img src=\"./bp.jpg\" width=\"480\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对训练例 ($\\boldsymbol{x}_k$, $\\boldsymbol{y}_k$)，假定神经网络的输出为 $\\hat{\\boldsymbol{y}}_k = (\\hat{y}_1^k, \\hat{y}_2^k, \\ldots, \\hat{y}_l^k)$，即\n",
    "\n",
    "$$ \\hat{y}_j^k = f(\\beta_j - \\theta_j) $$\n",
    "\n",
    "则网络在 ($\\boldsymbol{x}_k$, $\\boldsymbol{y}_k$) 上的均方误差为\n",
    "\n",
    "$$ E_k = \\frac{1}{2} \\sum_{j=1}^{l} \\left( \\hat{y}_j^k - y_j^k \\right)^2 $$\n",
    "\n",
    "我们的目的就是最小化上面的均方误差。\n",
    "\n",
    "BP算法基于梯度下降（gradient descent）策略，以目标的负梯度方向对参数进行调整：初始时随机选择所有参数，然后根据梯度下降方法对参数进行更新，直到满足一定的停止条件。我们要求解的参数有 $v_{ih}$（d×q个）、$w_{hj}$（q×l个）、$\\gamma_h$（q\n",
    "个）、$\\theta_j$（l个），任意参数v的更新公式为\n",
    "\n",
    "$$ v \\leftarrow v + \\Delta v $$\n",
    "\n",
    "下面以 $w_{hj}$ 为例进行推导。根据梯度下降，有\n",
    "\n",
    "$$ \\Delta w_{hj} = - \\eta \\frac{\\partial E_k}{\\partial w_{hj}} $$\n",
    "\n",
    "注意到 $w_{hj}$ 先影响到第j个输出层神经元的输入值 $\\beta_j$，再影响到其输出值 $\\hat{y}_j^k$，然后影响到 $E_k$，有\n",
    "\n",
    "$$ \\frac{\\partial E_k}{\\partial w_{hj}} = \\frac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\frac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} \\cdot \\frac{\\partial \\beta_j}{\\partial w_{hj}}$$\n",
    "\n",
    "首先，需要引入Sigmoid函数的一个性质：\n",
    "\n",
    "$$ f^{\\prime}(x) = f(x)(1-f(x)) $$\n",
    "\n",
    "于是\n",
    "\n",
    "$$ \\begin{align*} \n",
    "\\frac{\\partial E_k}{\\partial \\hat{y}_j^k} &= \\hat{y}_j^k - y_{j}^{k} \\\\\n",
    "\\frac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} &= f^{\\prime}(\\beta_j - \\theta_j) \\\\\n",
    "&= f(\\beta_j - \\theta_j)(1-f(\\beta_j - \\theta_j)) \\\\\n",
    "&= \\hat{y}_j^k (1-\\hat{y}_j^k) \\\\\n",
    "\\frac{\\partial \\beta_j}{\\partial w_{hj}} &= b_h\n",
    "\\end{align*}$$\n",
    "\n",
    "将上面的式子带入更新公式，得到\n",
    "\n",
    "$$ \\Delta w{hj} = \\eta \\hat{y}_j^k (1-\\hat{y}_j^k) (y_{j}^{k} - \\hat{y}_j^k) b_h $$\n",
    "\n",
    "令 $g_j = \\hat{y}_j^k (1-\\hat{y}_j^k) (y_{j}^{k} - \\hat{y}_j^k)$，则\n",
    "\n",
    "$$ \\Delta w_{hj} = \\eta g_j b_h $$\n",
    "\n",
    "类似可得\n",
    "\n",
    "$$ \\begin{align*} \n",
    "\\Delta \\theta_j &= - \\eta g_j \\\\\n",
    "\\Delta v_{ih} &= \\eta e_h x_i \\\\\n",
    "\\Delta \\gamma_h &= - \\eta e_h\n",
    "\\end{align*} $$\n",
    "\n",
    "其中\n",
    "\n",
    "$$ \\begin{align*} \n",
    "e_h &= - \\frac{\\partial E_k}{\\partial b_h} \\cdot \\frac{\\partial b_h}{\\partial \\alpha_h} \\\\\n",
    "&= - \\sum_{j=1}^{q} \\frac{\\partial E_k}{\\partial \\beta_j} \\cdot \\frac{\\partial \\beta_j}{\\partial b_h} f^{\\prime}(\\alpha_h - \\gamma_h) \\\\\n",
    "&= \\sum_{j=1}^{l} w_{hj} g_j f^{\\prime}(\\alpha_h - \\gamma_h) \\\\\n",
    "&= b_h (1 - b_h) \\sum_{j=1}^{l} w_{hj}g_j\n",
    "\\end{align*}$$\n",
    "\n",
    "学习率 $\\eta \\in (0, 1)$ 控制着算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛速度又会过慢。有事后为了做精细调节，可令 $ \\Delta w_{hj} $ 与 $\\Delta \\theta_j$ 使用 $\\eta_1$，$\\Delta v_{ih} $ 与 $\\Delta \\gamma_h$ 使用 $\\eta_2$，两者未必相等。\n",
    "\n",
    "下图给出了BP算法的工作流程。对每个训练样例，BP算法执行以下操作：先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；然后计算输出层的误差（第4-5行），再将误差逆向传播至隐层神经元（第6行），最后根据隐层神经元的误差来对连接权和阈值进行调整（第7行）。该迭代过程循环进行，直到达到某些停止条件为止，例如训练误差已达到一个很小的值。\n",
    "\n",
    "<img src=\"./bp2.jpg\" width=\"480\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
